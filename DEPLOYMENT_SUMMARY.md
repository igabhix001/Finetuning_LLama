# Finetuning_LLama - Ready for GitHub & RunPod

**Status:** âœ… Production-ready for Part 2 training  
**Date:** 2026-02-06  
**Hardware:** RTX 6000 Ada (48GB) â†’ RTX 3090 (24GB)  
**Embeddings:** OpenAI text-embedding-3-large (3072-dim)

---

## ðŸ“¦ What's Included

### Training Scripts (Production-Ready)
- âœ… `scripts/01_setup_environment.sh` - Environment setup
- âœ… `scripts/02_upload_pinecone.py` - RAG embeddings upload
- âœ… `scripts/03_train_dapt.py` - DAPT with LoRA (2-4 hours)
- âœ… `scripts/04_train_sft.py` - SFT with LoRA on DAPT (6-10 hours)
- âœ… `scripts/05_merge_adapters.py` - Merge DAPT + SFT LoRA
- âœ… `scripts/06_quantize_unsloth.py` - Quantize 8-bit with Unsloth
- âœ… `scripts/07_test_inference.py` - Test model
- âœ… `scripts/08_serve_vllm.py` - vLLM production server

### Configuration Files
- âœ… `configs/dapt_config.yaml` - DAPT training config
- âœ… `configs/dapt_lora_config.yaml` - DAPT LoRA parameters
- âœ… `configs/sft_config.yaml` - SFT training config
- âœ… `configs/lora_config.yaml` - SFT LoRA parameters

### Dataset Files (Already Copied)
- âœ… `data/dapt_corpus/` - 654 chunks, ~1.19M tokens
- âœ… `data/sft_train/` - 19,303 training examples
- âœ… `data/sft_validation/` - 398 validation examples
- âœ… `data/pinecone_upsert.jsonl` - 1,207 OpenAI embeddings (3072-dim)
- âœ… `data/kb_chunks.jsonl` - RAG chunks

### Documentation
- âœ… `README.md` - Complete overview
- âœ… `QUICKSTART.md` - 30-minute setup guide
- âœ… `GITHUB_SETUP.md` - GitHub push instructions
- âœ… `RUNPOD_SETUP.md` - Detailed RunPod guide
- âœ… `TRAINING_GUIDE.md` - Step-by-step training

### Environment Files
- âœ… `.env` - Your API keys (**gitignored**, not pushed to GitHub)
- âœ… `.env.example` - Template for RunPod (includes OpenAI key placeholder)
- âœ… `.gitignore` - Excludes models, checkpoints, logs, .env
- âœ… `requirements.txt` - All dependencies (incl. openai, vllm, unsloth)

---

## ðŸš€ Quick Start (3 Steps)

### 1. Push to GitHub
```powershell
cd d:\Dataset_preprossecing_pipeline\Finetuning_LLama
git init
git add .
git commit -m "Initial commit: KP Astrology training pipeline"
git remote add origin https://github.com/YOUR_USERNAME/Finetuning_LLama.git
git push -u origin main
```

### 2. Clone on RunPod
```bash
cd /workspace
git clone https://github.com/YOUR_USERNAME/Finetuning_LLama.git
cd Finetuning_LLama
cp .env.example .env
nano .env  # Add your keys
bash scripts/01_setup_environment.sh
```

### 3. Start Training
```bash
# LoRA DAPT
python scripts/03_train_dapt.py  # 2-4 hours

# LoRA SFT (on top of DAPT)
python scripts/04_train_sft.py   # 6-10 hours

# Merge adapters
python scripts/05_merge_adapters.py

# Quantize with Unsloth (8-bit)
python scripts/06_quantize_unsloth.py

# Test
python scripts/07_test_inference.py

# Serve with vLLM
python scripts/08_serve_vllm.py
```

---

## ðŸ“Š Training Pipeline

**Architecture:** Base Llama-3.1-8B-Instruct â†’ LoRA DAPT â†’ LoRA SFT â†’ Merge adapters â†’ Quantize (8-bit Unsloth) â†’ vLLM serve

| Phase | Script | Duration | Output |
|-------|--------|----------|--------|
| **LoRA DAPT** | `03_train_dapt.py` | 2-4 hrs | DAPT LoRA adapters |
| **LoRA SFT** | `04_train_sft.py` | 6-10 hrs | SFT LoRA adapters |
| **Merge** | `05_merge_adapters.py` | 30 mins | Full merged model |
| **Quantize** | `06_quantize_unsloth.py` | 30 mins | 8-bit quantized model |
| **Test** | `07_test_inference.py` | 5 mins | Quality validation |
| **Serve** | `08_serve_vllm.py` | - | vLLM production server |

**Total:** ~10-16 hours, ~$8-15 on RTX 6000 Ada

---

## ðŸ”§ Key Features

### Optimized for RTX 6000 Ada
- **LoRA-based training** (not full fine-tuning)
- FP16 training (not BF16)
- Gradient checkpointing enabled
- Memory-efficient optimizer (paged_adamw_32bit)
- Batch size: 4 with gradient accumulation 4 (effective: 16)
- DAPT LoRA + SFT LoRA stacked approach

### Production-Ready Code
- Comprehensive error handling
- Progress monitoring with TensorBoard
- Automatic checkpoint saving
- Resume capability on interruption
- Detailed logging

### Quality Assurance
- Validation during training
- Early stopping support
- Test inference script
- Quality metrics tracking

---

## ðŸ“ Directory Structure

```
Finetuning_LLama/
â”œâ”€â”€ README.md                    # Main documentation
â”œâ”€â”€ QUICKSTART.md               # 30-min setup guide
â”œâ”€â”€ GITHUB_SETUP.md             # GitHub instructions
â”œâ”€â”€ RUNPOD_SETUP.md             # RunPod detailed guide
â”œâ”€â”€ TRAINING_GUIDE.md           # Training instructions
â”œâ”€â”€ DEPLOYMENT_SUMMARY.md       # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Git exclusions
â”œâ”€â”€ .env                        # Your API keys (gitignored)
â”œâ”€â”€ .env.example                # Template
â”œâ”€â”€ configs/                    # Training configurations
â”‚   â”œâ”€â”€ dapt_config.yaml
â”‚   â”œâ”€â”€ sft_config.yaml
â”‚   â””â”€â”€ lora_config.yaml
â”œâ”€â”€ scripts/                    # Training scripts
â”‚   â”œâ”€â”€ 01_setup_environment.sh
â”‚   â”œâ”€â”€ 02_upload_pinecone.py
â”‚   â”œâ”€â”€ 03_train_dapt.py
â”‚   â”œâ”€â”€ 04_train_sft.py
â”‚   â”œâ”€â”€ 05_merge_lora.py
â”‚   â”œâ”€â”€ 06_quantize_model.py
â”‚   â””â”€â”€ 07_test_inference.py
â”œâ”€â”€ data/                       # Datasets (gitignored contents)
â”‚   â”œâ”€â”€ dapt_corpus/
â”‚   â”œâ”€â”€ sft_train/
â”‚   â”œâ”€â”€ sft_validation/
â”‚   â”œâ”€â”€ pinecone_upsert.jsonl
â”‚   â””â”€â”€ kb_chunks.jsonl
â”œâ”€â”€ models/                     # Model storage (gitignored)
â”œâ”€â”€ checkpoints/                # Training checkpoints (gitignored)
â””â”€â”€ logs/                       # Training logs (gitignored)
```

---

## ðŸ”‘ API Keys Required

On RunPod, copy `.env.example` to `.env` and add your keys:

```bash
# Pinecone (for RAG)
PINECONE_API_KEY=your-pinecone-key
PINECONE_ENVIRONMENT=us-east-1-aws
PINECONE_INDEX_NAME=kp-astrology-kb

# OpenAI (for RAG query embeddings)
OPENAI_API_KEY=your-openai-key

# HuggingFace (for Llama 3.1 access)
HF_TOKEN=your-hf-token
```

> **Note:** `.env` is gitignored. You must create it on RunPod from `.env.example`.

---

## ðŸ’° Cost Estimate

### Training on RTX 6000 Ada (~$0.89/hr)
- LoRA DAPT: 2-4 hours = $1.78-$3.56
- LoRA SFT: 6-10 hours = $5.34-$8.90
- Merge + Quantize: 1 hour = $0.89
- **Total: $8-14**

### Deployment on RTX 3090 (~$0.34/hr)
- Quantized model: ~8-10GB (8-bit Unsloth)
- vLLM serving: 50-100 tokens/sec
- OpenAI-compatible API
- Cost-effective for production

---

## âœ… Pre-Push Checklist

Before pushing to GitHub:

- [x] All scripts created and tested
- [x] Configuration files complete
- [x] Dataset files copied to data/
- [x] .gitignore configured correctly
- [x] .env.example created (without real keys)
- [x] Documentation complete
- [x] requirements.txt includes all dependencies
- [x] Directory structure clean

---

## ðŸŽ¯ What Happens on RunPod

1. **Clone:** Get all scripts and configs from GitHub
2. **Setup:** Install dependencies, configure environment
3. **Train DAPT:** Adapt model to KP astrology (2-4 hrs)
4. **Train SFT:** Instruction tuning with LoRA (6-10 hrs)
5. **Merge:** Combine LoRA with base model (30 mins)
6. **Quantize:** Compress to 4-bit for RTX 3090 (1 hr)
7. **Test:** Validate model quality (5 mins)
8. **Download:** Get quantized model for deployment

---

## ðŸ“š Documentation Guide

- **New to RunPod?** â†’ Start with `QUICKSTART.md`
- **Need detailed setup?** â†’ Read `RUNPOD_SETUP.md`
- **Want step-by-step training?** â†’ Follow `TRAINING_GUIDE.md`
- **GitHub questions?** â†’ Check `GITHUB_SETUP.md`
- **Quick reference?** â†’ See `README.md`

---

## ðŸ†˜ Support

### Common Issues

**Out of Memory:**
- Reduce batch size in config files
- Enable gradient checkpointing (already enabled)
- Use smaller LoRA rank

**Training Interrupted:**
- Training auto-resumes from checkpoints
- Checkpoints saved every 100/500 steps

**Dataset Missing:**
- Dataset files are in local repo
- Upload separately if needed (see GITHUB_SETUP.md)

**HuggingFace Access Denied:**
- Request access at huggingface.co/meta-llama/Llama-3.1-8B-Instruct
- Verify HF_TOKEN in .env

---

## ðŸŽ“ Training Metrics to Monitor

### DAPT
- **Loss:** Should decrease from ~3.5 to ~2.5
- **Perplexity:** Should reduce by 30-40%

### SFT
- **Train Loss:** Should decrease from ~2.5 to ~1.2
- **Eval Loss:** Should plateau around 1.3-1.6
- **Quality:** Check rule citations and KP terminology

---

## ðŸš€ Ready to Deploy!

**Everything is set up and ready for GitHub push.**

Next steps:
1. Review `QUICKSTART.md` for 30-minute setup
2. Push to GitHub using commands above
3. Launch RunPod RTX 6000 Ada pod
4. Clone and start training

**Total time to trained model: ~10-16 hours**  
**Total cost: ~$8-15**

---

**Good luck with training!** ðŸŽ¯
