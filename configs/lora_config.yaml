# LoRA Configuration for SFT
# Optimized for RTX 6000 Ada (48GB VRAM)

# LoRA parameters
r: 16  # Rank - higher = more parameters but better quality
lora_alpha: 32  # Scaling factor (typically 2x rank)
lora_dropout: 0.05

# Target modules for Llama 3.1
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# LoRA settings
bias: "none"
task_type: "CAUSAL_LM"
inference_mode: false

# Memory optimization
modules_to_save: null  # Don't save additional modules
