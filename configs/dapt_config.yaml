# DAPT Training Configuration for RTX 6000 Ada (48GB VRAM)
# Domain-Adaptive Pre-Training with LoRA on KP Astrology corpus

# Model paths
model_name: "meta-llama/Llama-3.1-8B-Instruct"  # Will download from HF
output_dir: "./checkpoints/dapt_lora/"
logging_dir: "./logs/dapt/"

# Dataset
train_data: "./data/dapt_corpus/"

# Training hyperparameters
num_train_epochs: 1
per_device_train_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size: 16
learning_rate: 2.0e-4  # Higher LR for LoRA
warmup_steps: 100
weight_decay: 0.01

# Logging and checkpointing
logging_steps: 10
save_steps: 100
save_total_limit: 3
eval_steps: 500

# Optimization for RTX 6000 Ada
fp16: true
bf16: false  # Use fp16 for Ada architecture
gradient_checkpointing: true
optim: "adamw_torch"
lr_scheduler_type: "cosine"
max_grad_norm: 1.0

# Context and tokenization
max_seq_length: 2048
dataloader_num_workers: 4
dataloader_pin_memory: true

# Memory optimization
gradient_checkpointing_kwargs:
  use_reentrant: false

# Reporting
report_to: "tensorboard"
